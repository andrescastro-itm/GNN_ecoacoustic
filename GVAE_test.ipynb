{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "\n",
    "from GraphDataset import MyDataset\n",
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load soft labels with membership level to each cover\n",
    "\n",
    "df_data = pd.read_csv('data/ReyZamuro_softLabels.csv',index_col=0)\n",
    "df_data = df_data.drop('RZUB02')\n",
    "df_data = df_data.drop('RZUA03b')\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of ARUs and labels\n",
    "\n",
    "DatosN = list(df_data.index)\n",
    "Clases = df_data.values.argmax(1)\n",
    "etiquetasN = Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 'YAMNet'#'PANNs'#'YAMNet' #'VGGish'#'AI'\n",
    "\n",
    "train_dataset = MyDataset(ListaArchivos=DatosN,\n",
    "                          etiquetas=etiquetasN, caract=features)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_data = [train_dataset[i][0] for i in range(len(train_dataset))]\n",
    "x = torch.stack(unpacked_data, dim=0).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import edge_creation_nodeinfo, is_connected, edge_creation_geoDistance, plot_distance_matrix_heatmap, edge_creation_coverinfo\n",
    "from torch_geometric.utils import is_undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = edge_creation_coverinfo(torch.tensor(df_data.values), x,'knn', k_neigh=11)\n",
    "for i in graphs:\n",
    "    print(f\"Is the graph {i} connected? {is_connected(i)}\")\n",
    "    print(f'Is the graph undirected {is_undirected(i.edge_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(graphs, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear modelo y entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixGCNVAE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, latent_dim, num_nodes):\n",
    "        super(MatrixGCNVAE, self).__init__()\n",
    "        \n",
    "        # Encoder components\n",
    "        self.conv1d = torch.nn.Conv1d(1, 64, 24, stride=24)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        \n",
    "        # Mean and log variance layers for the latent distribution\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.fc_mu = GCNConv(hidden_channels, latent_dim)\n",
    "        self.fc_logvar = GCNConv(hidden_channels, latent_dim)\n",
    "        \n",
    "        # Decoder components for node features\n",
    "        self.decoder_fc1 = torch.nn.Linear(latent_dim, hidden_channels)\n",
    "        self.num_rec = int(in_channels*24/64)\n",
    "        self.decoder_fc2 = torch.nn.Linear(hidden_channels, self.num_rec)\n",
    "        \n",
    "        # Decoder components for adjacency matrix\n",
    "        self.num_nodes = num_nodes\n",
    "        self.adj_decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, num_nodes)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x, edge_index):\n",
    "        # Process input features\n",
    "        x = x.transpose(1, 2).flatten(1)  # Flatten the matrix features\n",
    "        x = self.conv1d(x.unsqueeze(1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # GCN encoding\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Get latent distribution parameters\n",
    "        x = self.bn(x)\n",
    "        mu = self.fc_mu(x, edge_index)\n",
    "        logvar = self.fc_logvar(x, edge_index)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        return mu + torch.randn_like(logvar) * torch.exp(logvar)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        # Decode node features from latent space\n",
    "        h = F.relu(self.decoder_fc1(z))\n",
    "        node_reconstruction = self.decoder_fc2(h)\n",
    "        \n",
    "        # Decode adjacency matrix from latent space\n",
    "        adj_logits = self.adj_decoder(z)\n",
    "        # Create adjacency predictions - a matrix where each row i contains scores for edges from node i to all nodes\n",
    "        adj_matrix = torch.sigmoid(torch.matmul(adj_logits, adj_logits.transpose(0, 1)))\n",
    "        \n",
    "        return node_reconstruction, adj_matrix\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Encode to get latent distribution\n",
    "        mu, logvar = self.encode(x, edge_index)\n",
    "        # mu, logvar = torch.tanh(mu), torch.tanh(logvar)\n",
    "        # Sample from the latent distribution\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        # print(z)\n",
    "        # Decode to get reconstructions\n",
    "        node_reconstruction, adj_reconstruction = self.decode(z)\n",
    "        \n",
    "        return node_reconstruction, adj_reconstruction, mu, logvar\n",
    "    \n",
    "    def edge_index_to_adj_matrix(self, edge_index, num_nodes):\n",
    "        \"\"\"Convert edge_index to dense adjacency matrix\"\"\"\n",
    "        adj_matrix = torch.zeros((num_nodes, num_nodes), device=edge_index.device)\n",
    "        adj_matrix[edge_index[0], edge_index[1]] = 1.0\n",
    "        return adj_matrix\n",
    "    \n",
    "    def loss_function(self, node_reconstruction, adj_reconstruction, x_original, edge_index, mu, logvar, alpha=1.0, beta=1.0):\n",
    "        # Node feature reconstruction loss (MSE)\n",
    "        # print(f\"true reconstructed= {node_reconstruction.shape}, original={x_original.transpose(1, 2).flatten(1).shape}\")\n",
    "        feature_loss = F.mse_loss(node_reconstruction, x_original.transpose(1, 2).flatten(1))\n",
    "        \n",
    "        # Adjacency matrix reconstruction loss (BCE)\n",
    "        true_adj = self.edge_index_to_adj_matrix(edge_index, self.num_nodes)\n",
    "        \n",
    "        # Binary cross entropy for adjacency matrix\n",
    "        # We can add class weights if the graph is sparse\n",
    "        # print(f\"{adj_reconstruction.double()=}\")\n",
    "        # print(f\"{true_adj.double()=}\")\n",
    "        adj_loss = F.binary_cross_entropy(adj_reconstruction.double(), true_adj.double())\n",
    "        \n",
    "        # KL Divergence loss\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        # Total loss with weighting factors\n",
    "        total_loss = feature_loss + alpha * adj_loss + beta * kl_loss\n",
    "        \n",
    "        return total_loss, feature_loss, adj_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nodes = max([data.num_nodes for data in graphs])\n",
    "num_feat = graphs[0].x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatrixGCNVAE(in_channels=64*num_feat, hidden_channels=num_feat, latent_dim=8, num_nodes=max_nodes)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) #0.01 GCN\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    model.double()\n",
    "    for graph in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        node_reconstruction, adj_reconstruction, mu, logvar = model(graph.x.double(), graph.edge_index)\n",
    "        loss, feature_loss, adj_loss, kl_loss = model.loss_function(node_reconstruction, adj_reconstruction, graph.x.double(),graph.edge_index, mu, logvar, alpha=1.0, beta=0.1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for graph in train_loader:\n",
    "        _, _, mu, logvar = model(graph.x.double(), graph.edge_index)\n",
    "        embeddings = model.reparameterize(mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.shape)\n",
    "embeddings = (embeddings - embeddings.min(0).values)/(embeddings.max(0).values - embeddings.min(0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = pd.read_csv('./data/ReyZamuro_latlon.csv',index_col='field_numb')\n",
    "df_map = df_map.drop('RZUA03b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = embeddings.numpy()\n",
    "sim_mat = (embs@embs.T)\n",
    "sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norms = np.linalg.norm(embs, axis=1)\n",
    "# sim_mat = embs @ embs.T / (norms[:, np.newaxis] @ norms[np.newaxis, :])\n",
    "# sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import edge_index_to_adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_graph = edge_index_to_adjacency(graph.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.triu(sim_mat, k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(121)\n",
    "sns.heatmap(np.triu(adj_graph, k=1), cmap=\"YlOrRd\", square=True, cbar_kws=dict(use_gridspec=False,location=\"right\",pad=0.01,shrink=0.5))\n",
    "plt.title('Adjacency matrix of graph')\n",
    "plt.xlabel(\"ARU Index\")\n",
    "plt.ylabel(\"ARU Index\")\n",
    "plt.subplot(122)\n",
    "sns.heatmap(np.triu(sim_mat, k=1), cmap=\"YlOrRd\", square=True, cbar_kws=dict(use_gridspec=False,location=\"right\",pad=0.01,shrink=0.5))\n",
    "plt.title('Simmilarity Matrix')\n",
    "plt.xlabel(\"ARU Index\")\n",
    "plt.ylabel(\"ARU Index\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distance_matrix_heatmap(adj_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distance_matrix_heatmap(sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density metric:\n",
    "\n",
    "print(np.mean(sim_mat)) #axis = 0: por nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connectivity metric (only if thresholded):\n",
    "\n",
    "print(np.count_nonzero(sim_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total weight (es density sin promediar)\n",
    "\n",
    "print(np.sum(sim_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floyd_warshall(matrix):\n",
    "    \"\"\"Implement Floyd-Warshall algorithm for all-pairs shortest paths\"\"\"\n",
    "    n = len(matrix)\n",
    "    dist = np.array(matrix, dtype=float)\n",
    "    \n",
    "    # Replace inf with large number for calculations\n",
    "    dist[dist == float('inf')] = 1e9\n",
    "    \n",
    "    for k in range(n):\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = floyd_warshall(sim_mat)\n",
    "plot_distance_matrix_heatmap(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diameter\n",
    "print(np.max(dist_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average path\n",
    "print(np.mean(dist_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Betweeness centrality\n",
    "\n",
    "n = len(dist_matrix)\n",
    "centrality = np.zeros(n)\n",
    "for s in range(n):\n",
    "    for t in range(n):\n",
    "        if s != t:\n",
    "            # Count shortest paths going through each vertex\n",
    "            for v in range(n):\n",
    "                if v != s and v != t:\n",
    "                    if dist_matrix[s][t] == dist_matrix[s][v] + dist_matrix[v][t]:\n",
    "                        centrality[v] += 1\n",
    "value = np.mean(centrality)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchGeometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
